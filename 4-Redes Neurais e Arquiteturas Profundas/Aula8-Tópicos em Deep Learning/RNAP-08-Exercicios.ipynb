{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VIII -  Tópicos em Deep Learning</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Exercícios</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:red\">Recomenda-se fortemente que os exercícios sejam feitos sem consultar as respostas antecipadamente.</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Exercícios Essenciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Exercício 1)\n",
    "\n",
    "Leia sobre o dataset e desafio COCO Keypoints em https://cocodataset.org/#keypoints-2020, no qual o objetivo é detectar pessoas numa cena e localizar seus pontos chave.\n",
    "\n",
    "Qual abordagem, dentre as abaixo, seria adequada para esse cenário?\n",
    "\n",
    " (a) Rede neural convolucional treinada para detectar a presença de uma ou mais pessoas, bem como realizar regressão dos pontos chave de cada pessoa por meio do projeto de uma camada de saída com essas informações<br>\n",
    " (b) Rede neural convolucional para classificar entre imagens que possuem pessoas e imagens que não possuem pessoas, aprendendo a detecção de pessoas por meio de uma função de entropia cruzada a qual deverá computar a probabilidade de uma ou mais pessoas estarem na cena.<br>\n",
    " (c) Rede neural convolucional com função de perda triplet aproximar pontos chave de uma pessoa independente de sua pose<br>\n",
    " (d) Rede neural densa com emprego de uma função de custo que combine classificação e intersecção sobre união para gerar como saída a sobreposição entre áreas ocupadas por pessoas preditas e reais<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 2)\n",
    "\n",
    "Assuma um cenário em que temos uma grande base de dados com documentos, em formato texto, escrito por 200 autores de uma editora. Para fins de detecção de plágio queremos aprender uma representação que seja capaz de projetar um texto num espaço de características (espaço latente) no qual seja possível medir a similaridade entre esse texto de entrada e um texto da base de dados, identificando de qual autor possui maior similaridade. Como principal problema, o texto possui alta sobreposição de conteúdo com trechos em comum entre autores arbitrários, sendo que os trechos que caracterizam cada autor representa parte significativamente menor do texto, o que é difícil de detectar e pré-processar (impedindo por exemplo remover esses trechos de confusão).\n",
    "\n",
    "Tendo em mãos um word-embedding adequado, qual seria a melhor opção para aprender uma representação útil, considerando as abordagens:\n",
    "\n",
    "I - Camada GRU<br>\n",
    "II - Camada densa de projeção em k dimensões<br>\n",
    "III - Camada densa de classificação com 200 neurônios<br>\n",
    "IV - Função entropia cruzada<br>\n",
    "V - Função contrastiva<br>\n",
    "\n",
    " (a) Treinar rede com camadas I, II, III e função V<br>\n",
    " (a) Treinar rede com camada II e função IV<br>\n",
    " (c) Treinar rede com camadas I, II e III com função IV, e depois novamente as camadas I e II e a função V, retirando a IV<br>\n",
    " (d) Treinar rede com camadas I e III com função V, e depois acrescentar a camadas II treinando com a função IV<br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 3)\n",
    "\n",
    "Redes neurais que tratam de problemas mais complexos como regressão com classificação, uso de múltiplos fluxos, entre outros, comumente especificam estratégias de treinamento para permitir o aprendizado ainda que sob condições mais difíceis. No caso da detecção de objetos em imagens em que é preciso localizar (via regressão) um objeto, e classificá-lo, qual conceito é fundamental para computar a confiança da saída da rede para uma determinada instância e facilitar a interpretação do resultado? \n",
    "\n",
    " (a) Intersecção sobre união: permitindo que a rede neural tenha uma tolerância na sobreposição entre a localização verdadeira e a predita.<br>\n",
    " (b) Uso de uma grade regular de células: permitindo que um mesmo objeto seja detectado em diferentes partes da imagem<br>\n",
    " (c) Supressão de não máximos: remover detecções com baixa probabilidade e assim reduzir a quantidade de falsos positivos ou objetos detectados de forma duplicada<br>\n",
    " (d) Definir âncoras: caixas (bounding boxes) com aspecto esperado com relação à sua altura e largura.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercício 4)\n",
    "\n",
    "Para qual cenário a auto-supervisão é uma técnica útil?\n",
    "\n",
    " (a) Processar dados não estruturados de qualquer origem criando um classificador diretamente a partir dos dados sem a necessidade de rotulação, e posteriormente classificar novas imagens utilizando esse modelo.<br>\n",
    " (b) Pré-processar grandes bases de dados de forma a eliminar outliers, ruído e outros efeitos indesejados nessa base de dados, garantindo maior acurácia para dados futuros.<br>\n",
    " (c) Coletar e utilizar grandes quantidades de dados não anotados, e obter um modelo inicial que possa ser utilizado como base para adaptar modelos para problemas específicos, vencendo a barreira de anotar dados massivamente.<br>\n",
    " (d) Para base de dados pequenas em que não se dispõe de grande quantidades de exemplos para treinar redes neurais profundas para tarefas de classificação e regressão, pois essa técnica realiza aumento de dados de forma nativa.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfnm0YgLyqqU"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 5)\n",
    "\n",
    "Suponha uma base de dados que possua 32 características relacionadas a transações financeiras realizadas por clientes de um banco. Temos disponível 10 milhões de transações não anotadas de diferentes clientes, e transações anotadas para 20 mil clientes: em número girando entre 100 e 2 mil transações anotadas em \"normais\", \"suspeitas\" e \"anomalias\" por cliente. \n",
    "\n",
    "Desejamos criar modelos de detecção de anomalias em transações para clientes recentes do banco, para os quais temos nenhum ou poucos dados de transações. Para evitar que as primeiras predições sejam aleatórias, considere as seguintes abordagens para aprender uma representação com 32 características que possa ser usada como modelo inicial para posterior ajuste com dados futuros rotulados:\n",
    "\n",
    "I - denoising autoencoder overcomplete com restrição de esparsidade no código de 32 dimensões<br>\n",
    "II - rede neural que receba por entrada as 32 dimensões embaralhadas e que aprenda a detectar as posições corretas das características<br>\n",
    "III - rede neural treinada com transações anotadas de clientes com perfil semelhante ao novo cliente<br>\n",
    "\n",
    "Essas abordagens dizem respeito a:\n",
    "\n",
    "(a) I - auto-supervisão, II - análise de permutações, III - aprendizado ativo<br>\n",
    "(b) I - aprendizado não-supervisionado, II e III - auto-supervisão<br>\n",
    "(c) I - redução de dimensionalidade, II - auto-supervisão, III - transferência de aprendizado<br>\n",
    "(d) I e II - auto-supervisão, III - transferência de aprendizado<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 6)\n",
    "\n",
    "Quais dos itens abaixo foram contribuições inovadores do método BERT?\n",
    "\n",
    "I - método de pré-treinamento de modelos<br>\n",
    "II - propor um novo módulo/bloco de rede neural<br>\n",
    "III - uso de mecanismo de atenção bidirecional<br>\n",
    "IV - permitir ajuste-fino do modelo para diversas tarefas<br>\n",
    "\n",
    "(a) I, II, IV<br>\n",
    "(b) I, III, IV<br>\n",
    "(c) II, III, IV<br>\n",
    "(d) I, II<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 7)\n",
    "\n",
    "Suponha um problema em que temos mapas diários codificados por uma matriz com $200 \\times 200$ pontos com dados de sensores da temperatura média coletada por dia. Temos uma base de dados contendo 3000 mapas completos e preenchidos em todas as células/elementos. Os mapas de treinamento foram coletados de forma arbitrária, não sequencial, e não temos dados sobre o momento/dia em que foram coletados.\n",
    "\n",
    "Qual dos métodos/arquiteturas abaixo pode permitir preencher dados faltantes nesse tipo de problema?\n",
    "\n",
    "(a) Yolo<br>\n",
    "(b) LSTM<br>\n",
    "(c) Rede contrastiva siamesa<br>\n",
    "(d) UNet<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2: Exercício Complementar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Exercício 8)\n",
    "\n",
    "Vamos implementar as funções contrastive e triplet para observar se essas funções são minimizadas após o treinamento de uma CNN com a camada softmax e entropia cruzada. Em resumo, não iremos treinar a rede com as funções contrastive/triplet, mas apenas avaliar o espaço de características.\n",
    "\n",
    "Use as implementações abaixo. Elas foram feitas para maior entendimento e não para velocidade. Caso necessário, o ponto principal de melhoria é a função `np.where()` que poderia ser substituída por algo mais eficiente.\n",
    "\n",
    "Carregue a base de dados Fashion MNIST e monte uma CNN com a seguinte arquitetura:\n",
    "* Convolucional com 32 filtros 3x3 e strides (2,2), sem zero pading, ativação relu\n",
    "* Convolucional com 64 filtros 3x3 e strides (2,2), sem zero pading, ativação relu\n",
    "* Convolucional com 64 filtros 3x3 e strides (2,2), sem zero pading, ativação relu\n",
    "* Densa com 64 neurônios e ativação relu (embedding)\n",
    "* Dropout 0.25\n",
    "* Densa softmax com 10 neurônios\n",
    "\n",
    "Defina as sementes seed(1), set_seed(2) e compile o modelo com o otimizador Adam, taxa de aprendizado 0.001. Após isso, monte um modelo que gere como saída a camada Densa de 64 neurônios (embedding). Obtenha as representações do embedding (antes do treinamento) para os 50 primeiros exemplos de treinamento. Depois:\n",
    "1. Defina random.seed(1)\n",
    "2. Compute e armazene as funções contrastive e triplet nesses 50 exemplos: note que é preciso passar as classes (em formato numérico) e as representações para a função. \n",
    "\n",
    "Depois, com 20 épocas e batchsize = 32, treine com o otimizador Adam, taxa de aprendizado 0.01, e usando apenas as 10000 primeiras instâncias da base [:10000].\n",
    "\n",
    "Obtenha as representações do embedding após o treinamento para os 50 primeiros exemplos de treinamento. A seguir:\n",
    "1. Defina random.seed(1)\n",
    "2. Compute e armazene as funções contrastive e triplet nas representações dos mesmos 50 exemplos usando o modelo treinado\n",
    "\n",
    "Considerando os valores das perdas arredondados para 4 casas decimais e o efeito do treinamento nas perdas contrastive e triplet, qual foi o resultado?\n",
    "\n",
    "OBS: para que essas funções possam ser usadas para treinamento no Keras é preciso utilizar funções do tensorflow ao invés de numpy, além de garantir que o `shape` de `y_true` é igual a de `y_pred`. Caso necessário, adapte e procure como passar essa função de perda personalizada.\n",
    "\n",
    " (a) Valor da triplet é maior do que o da contrastiva, e apenas a triplet reduziu seu valor significativamente, a contrastive teve redução na 4.a casa decimal<br>\n",
    " (b) Valor da triplet é menor do que o da contrastiva, e ambas foram minimizadas significativamente (na ordem da 2.a casa decimal ou maior) após o treinamento<br>\n",
    " (c) Valor da triplet é maior do que o da contrastiva, e ambas foram minimizadas significativamente (na ordem da 2.a casa decimal ou maior) após o treinamento<br>\n",
    " (d) Valor da triplet é menor do que o da contrastiva, e apenas a contrastive reduziu seu valor significativamente, a triplet teve redução na 4.a casa decimal<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred):\n",
    "    \"\"\" Triplet loss\n",
    "        y_true: classes dos exemplos de forma numérica (não one-hot-encoding)\n",
    "        y_pred: predicao da rede em termos das *representacoes*/embedding aprendida\n",
    "        Moacir A. Ponti/2020\n",
    "    \"\"\"\n",
    "    L = 0 # loss acumulada\n",
    "    m = 1 # margem\n",
    "    size = y_true.shape[0] # tamanho do conjunto a avaliar\n",
    "    \n",
    "    # normalizar dados 0-1\n",
    "    y_pred = (y_pred-np.min(y_pred))/(np.max(y_pred)-np.min(y_pred))\n",
    "    \n",
    "    # seleciona uma ancora por exemplo no conjunto\n",
    "    for i in range(size):\n",
    "        a = y_pred[i] # representacao da ancora\n",
    "        y = y_true[i] # classe da ancora\n",
    "        \n",
    "        # seleciona exemplo aleatorio, positivo e negativo\n",
    "        pos_ind = np.where(y_true == y)\n",
    "        neg_ind = np.where(y_true != y)\n",
    "        p = y_pred[pos_ind[0][random.randint(0,len(pos_ind[0])-1)]]\n",
    "        n = y_pred[neg_ind[0][random.randint(0,len(neg_ind[0])-1)]]\n",
    "        \n",
    "        # computa triplet loss\n",
    "        lapn = np.max([0, m + np.sum(np.power(a-p,2)) - np.sum(np.power(a-n,2))])/2.0\n",
    "        # acumula\n",
    "        L += lapn\n",
    "\n",
    "    return L/float(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    \"\"\" Contrastive loss\n",
    "        y_true: classes dos exemplos de forma numérica (não one-hot-encoding)\n",
    "        y_pred: predicao da rede em termos das *representacoes*/embedding aprendida\n",
    "        Moacir A. Ponti/2020\n",
    "    \"\"\"    \n",
    "    L = 0\n",
    "    m = 1\n",
    "    size = y_true.shape[0]\n",
    "\n",
    "    # normalizar dados\n",
    "    y_pred = (y_pred-np.min(y_pred))/(np.max(y_pred)-np.min(y_pred))\n",
    "\n",
    "    # para cada exemplo \"ancora\"\n",
    "    for i in range(size):\n",
    "        a = y_pred[i] # representacao a\n",
    "        y_a = y_true[i] # classe de a\n",
    "        \n",
    "        # sorteia exemplo (nao ancora)\n",
    "        ind = i \n",
    "        while (ind == i):\n",
    "            ind = random.randint(0,size-1)\n",
    "        \n",
    "        p = y_pred[ind] # representacao p\n",
    "        \n",
    "        # calcula contrastive loss (com base na classe)\n",
    "        if (y_true[ind] == y_a):\n",
    "            lapn = np.sum(np.power(a-p,2))/2.0\n",
    "        else: \n",
    "            lapn = np.max([0, m - np.sum(np.power(a-p, 2)) ])/2.0\n",
    "            \n",
    "        L += lapn\n",
    "        \n",
    "    return L/float(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 22:02:07.615705: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando datasets do keras\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "Classes:  10\n",
      "Tamanho da entrada:  (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# obtendo informações das imagens (resolucao) e dos rótulos (número de classes)\n",
    "img_lin, img_col = x_train.shape[1], x_train.shape[2]\n",
    "if (len(x_train.shape) == 3):\n",
    "      n_channels = 1\n",
    "else:\n",
    "      n_channels = x_train.shape[3]\n",
    "        \n",
    "num_classes = len(np.unique(y_train))\n",
    "print(x_train.shape)\n",
    "print('Classes: ', num_classes)\n",
    "\n",
    "# dividir por 255 para obter normalizacao\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# transformar categorias em one-hot-encoding\n",
    "y_train_one_hot = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_one_hot = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# formatar para treinamento\n",
    "x_train = x_train.reshape(x_train.shape[0], img_lin, img_col, n_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_lin, img_col, n_channels)\n",
    "\n",
    "# formato da entrada\n",
    "input_shape = (img_lin, img_col, n_channels)\n",
    "print('Tamanho da entrada: ', input_shape)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Exercicios_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
