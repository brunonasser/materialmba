{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo III - Arquiteturas de CNNS e treinamento de redes profundas</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Exercícios</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:red\">Recomenda-se fortemente que os exercícios sejam feitos sem consultar as respostas antecipadamente.</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parte 1 - Exercícios Essenciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 1)\n",
    "\n",
    "Considere as funções de custo: Perda Quadrática (MSE), Erro Absoluto (MAE), Perda 0-1, Perda Hinge/SVM, Entropia Cruzada. Para referência veja a definição da Perda 0-1 e Hinge, não vistas em aula, abaixo. Pesquise mais sobre essas caso necessário\n",
    "\n",
    "*. Perda 0-1\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t0  & \\mbox{if } y_i = \\hat{y}_i \\\\\n",
    "\t\t1 & \\mbox{if } y_i \\neq \\hat{y}_i \n",
    "\t\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "*. Perda SVM/Hinge\n",
    " \n",
    "$$\\frac{1}{N} \\sum_{i=1}^N \\max(0, 1- y^{h}_i\\cdot f(x_i)),$$\n",
    "essa função considera que as classes são -1 e 1, sendo $f(x_i)=\\hat{y}_i^{h}$ um valor de saída considerando valores negativos (os quais gerarão classificação para a classe -1) e positivos (classificação para a classe 1). Portanto será preciso adaptar as classes do problema e a  saída $\\hat{y}^{h}$ para esse cenário da seguinte forma:\n",
    "* $y^{h} \\in \\{-1,1\\}$, e\n",
    "* $\\hat{y}^{h} = 2\\cdot(\\hat{y}-0.5)$,\n",
    "sendo $\\hat{y}$ a probabilidade de uma instância pertencer à classe positiva (1).\n",
    "\n",
    "Para um determinado problema, como escolher qual delas utilizar no treinamento de uma rede neural?\n",
    "\n",
    "(a) Na dúvida escolher sempre a entropia cruzada, pois é a mais popular e considerada um padrão na literatura da área de redes neurais<br>\n",
    "(b) É necessário considerar um subconjunto pequeno de exemplos e sempre testar todas as funções de custo disponíveis, só assim é possível ter certeza de que estaremos selecionando a função mais adequada para o problema em mãos<br>\n",
    "(c) Avaliar o problema em termos de suas saídas e os valores possíveis para a função de custo, selecionando para realizar experimentos aquelas que mais se adequem ao problema e seja conveniente para realizar otimização baseada em gradiente<br>\n",
    "(d) Em geral, a entropia cruzada deve ser utilizada para problemas de classificação, e a perda quadrática para problemas de regressão, não sendo necessário investigar outras funções de custo pois são mais relevantes outros parâmetros como a taxa de aprendizado e o tamanho do batch<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Exercício 2)\n",
    "\n",
    "Considerando as funções de perda: entropia cruzada categórica e perda quadrática, qual é o valor das perdas para um exemplo arbitrário no caso o modelo considere as classes equiprováveis numa tarefa de classificação de 5 classes?\n",
    "\n",
    " (a) Entropia Cruzada = 1.6; Quadrática = 0.8<br>\n",
    " (b) Entropia Cruzada = 2.3; Quadrática = 0.8<br>\n",
    " (c) Entropia Cruzada = 1.6; Quadrática = 0.16<br>\n",
    " (d) Entropia Cruzada = 0.32; Quadrática = 0.8<br>\n",
    "  \n",
    "DICA: compare dois vetores de probabilidade, um com a classe real em *one-hot-encoding* e o outro exemplificando o caso de classes equiprováveis.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "JmVBm5CByqqM",
    "outputId": "5d912e29-5f38-4038-f886-8b7b1ceee008"
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6exl-MrVyqqT"
   },
   "source": [
    "---\n",
    "### Exercício 3)\n",
    "\n",
    "Sobre os métodos de otimização, o que podemos dizer quando comparamos SGD e Adam?\n",
    "\n",
    " (a) Ambos realizam atualização iterativa dos parâmetros usando o gradiente, mas o Adam utiliza também o segundo momento do gradiente como ponderação<br>\n",
    " (b) O Adam pode ser considerado um caso particular do SGD, sendo ambos algoritmos idênticos se usado SGD com Momentum e atrito de 0,99.<br>\n",
    " (c) Ambos realizam atualização iterativa dos parâmetros usando o gradiente, mas apenas SGD permite decaimento da taxa de aprendizado <br>\n",
    " (d) O Adam sempre obterá resultados melhores do que o SGD para qualquer rede neural profunda<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfnm0YgLyqqU"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 4)\n",
    "\n",
    "Dentre as alternativas, escolha a prática válida mais relevante ao projetar o treinamento de redes profundas\n",
    "\n",
    "(a) Inicializar todos os pesos com valores aleatórios e utilizar o maior número de instâncias possíveis no treinamento, garantindo que os hiperparâmetros com valor padrão obterão bons resultados<br>\n",
    "(b) Utilizar sempre a função de custo entropia cruzada, para a qual é recomendado o uso do otimizador Adam e taxa de aprendizado com decaimento. Definir a melhor taxa de decaimento de forma a minimizar a diferença entre o custo de treinamento e validação<br>\n",
    "(c) Utilizar conjunto pequeno de instâncias para busca inicial de hiperparâmetros como: otimizador, taxa de aprendizado, momentum e tamanho de batch, e depois refinar a busca num conjunto maior com base em métricas obtidas nos conjuntos de validação e treinamento<br>\n",
    "(d) Rezar para Yan LeCun, Yoshua Bengio, Geoffrey Hinton e Kunihiko Fukushima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6VfUMk8yqqW"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 5)\n",
    "\n",
    "Qual a principal diferença das arquiteturas VGGNet, Inception e Residual Network com relação à suas camadas convolucionais?\n",
    "\n",
    "(a) A VGGNet possui camadas convolucionais com filtros de mesmo tamanho $3\\times3$, enquanto as outras arquiteturas, Inception e ResNet aplicam filtros $5\\times5$ ou com concatenação de mapas de ativação ao longo da rede<br>\n",
    "(b) A rede Inception permite treinamento com maior número de camadas quanto comparada à VGGNet, que por sua vez permite treinamento com maior número de camadas quanto comparada à ResNet <br>\n",
    "(c) A VGGNet possui camadas convolucionais sequenciais, eventualmente seguidas de MaxPooling, enquanto a ResNet computa mapas de ativação de com diferentes filtros, concatenando-os, e a Inception possui um módulo do tipo banco de filtros, que permite saltar para camadas futuras, facilitando o treinamento com mais camadas<br>\n",
    "(d) A VGGNet possui camadas convolucionais sequenciais, enquanto Inception possui camadas convolucionais paralelas, e ResNet tem mapas de ativação que desviam da lógica sequencial e pulam camadas<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 6)\n",
    "\n",
    "Utilizando a biblioteca Keras, investige os hiperparâmetros relacionadas a learning rate na base de dados Boston Housing. Carregue a base de dados e normalize os atributos com z-score. Crie uma rede com camadas densas: 16, 8 e 1 (de saída), todas com ativação `relu`, função de custo `mse`, medindo também a `mae` como avaliação adicional.\n",
    "\n",
    "Iremos investigar o uso de decaimento de learning rate, a partir de um valor inicial estabelecido. Para isso vamos usar um conjunto de validação de 20% retirado a partir do conjunto de testes, e repetir 5 vezes, cada vez utilizando uma semente, de 1 até 5, conforme código base abaixo.\n",
    "\n",
    "Treine por 25 épocas com batchsize 32 e com o otimizador Adam, 2 arquiteturas diferentes:\n",
    "\n",
    "*A*. Uso dos parâmetros padrão<br>\n",
    "*B*. Iniciando com learning rate 0.01 e decaimento exponencial de 0.05 a partir da época 5\n",
    "\n",
    "Posteriormente, treine os dois modelos, porém agora com o conjunto de treinamento completo e avalie no conjunto de teste.\n",
    "\n",
    "Considerando a média dos valores de erro (MSE e MAE) obtidos na validação arredondados para um número inteiro (ou seja, sem considerar as casas decimais), e posteriormente os mesmos erros quando treinado com o conjunto completo e avaliados no teste:\n",
    "\n",
    "(a) B obteve menores valores de erro (MSE e MAE) do que A na validação, B também tem MAE menor do que A no teste, mas ambos foram similares no MSE do teste<br>\n",
    "(b) B obteve menores valores de erro (MSE e MAE) do que A na validação e no teste<br>\n",
    "(c) A obteve valores de erro MAE menores do que B, mas valores MSE maiores do que B na validação e no teste.<br>\n",
    "(d) A obteve valores de erro MAE e MSE similares com B na validação e no teste<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5Owfr6GyqqY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "mean = x_train.mean(axis=0)\n",
    "std = x_train.std(axis=0)\n",
    "\n",
    "x_train -= mean\n",
    "x_train /= std\n",
    "\n",
    "x_test -= mean\n",
    "x_test /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "# arrays para conter os erros\n",
    "mses = []\n",
    "maes = []\n",
    "\n",
    "# para cada semente\n",
    "for sd in range(1,6):\n",
    "    #x_trains, x_val, y_trains, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=sd)\n",
    "    \n",
    "    print(\"Seed\", sd)\n",
    "    seed(sd)\n",
    "    set_seed(sd)\n",
    "    # treinar modelo A\n",
    "    \n",
    "    seed(sd)\n",
    "    set_seed(sd)\n",
    "    # treinar modelo B\n",
    "    \n",
    "    #avaliar na validacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avalia novamente, agora no teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_F2ODAmyqq4"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 7)\n",
    "\n",
    "Utilizando ainda a biblioteca Keras, investige o impacto do uso de parâmetros padrão de batchsize na base de dados Boston Housing, agora utilizando a mesma arquitetura da atividade anterior, com otimizador Adam, iniciando com learning rate 0.02 e decaimento exponencial de 0.05 a partir da época 6.\n",
    "\n",
    "Investige valores de batch = 2, 4, 8, 16, 32, 64, 128 e 256 executando por 15 épocas.\n",
    "\n",
    "Para isso vamos usar um conjunto de validação de 20% retirado a partir do conjunto de testes, e repetir 5 vezes, cada vez utilizando uma semente, de 1 até 5, conforme código base abaixo.\n",
    "\n",
    "Após o treinamento, avalie MSE nos dados de validação e imprima a média do MSE obtido para os diferentes valores de batchsize.\n",
    "\n",
    "Quais foram os dois piores e os dois melhores valores de tamanho de batch em termos do MSE de validação?\n",
    "\n",
    "(a) Piores: 128 e 256; Melhores: 8 e 16<br>\n",
    "(b) Piores: 16 e 64; Melhores: 32 e 64<br>\n",
    "(c) Piores: 2 e 4; Melhores: 8 e 16<br>\n",
    "(d) Piores: 2 e 256; Melhores: 16 e 32<br>\n",
    "\n",
    "OBS: a aleatoriedade do processo pode gerar alguma variação de resultado, caso seja isso tente escolher a alternativa mais coerente, ou rodar algumas vezes para ver se há alternativa válida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seu codigo preparacao para experimentos\n",
    "\n",
    "for sd in range(1,6):\n",
    "    seed(sd)\n",
    "    set_seed(sd)\n",
    "    # seu codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3zlC0i0yqrX"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 8)\n",
    "\n",
    "O que podemos concluir dos dois exercícios anteriores (7 e 8)?\n",
    "\n",
    "(a) Os valores padrão para os hiperparâmetros geram bons resultados. A busca por outros parâmetros pode não valer a pena pois a diferença alcançada observada é pequena.<br>\n",
    "(b) Devemos sempre utilizar Adam com decaimento de taxa de aprendizado e batch size de tamanho entre 8 e 64, sendo que o uso do padrão (32) é normalmente suficiente.<br>\n",
    "(c) Batchs de tamanho muito grande são prejudiciais ao treinamento, e o otimizador Adam é sempre melhor com decaimento de taxa de aprendizado.<br>\n",
    "(d) O uso de hiperparâmetros com valores padrão pode gerar resultados subótimos, sendo importante uma busca de parâmetros para melhor otimizar modelos<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parte 2 - Exercícios Complementares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    " ### Exercício 9)\n",
    "\n",
    "Considere 4 funções de custo distintas: 1. entropia cruzada binária, 2. perda quadrática, vistas em aula, e mais duas adicionais:\n",
    "\n",
    "3. Perda 0-1\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t0  & \\mbox{if } y_i = \\hat{y}_i \\\\\n",
    "\t\t1 & \\mbox{if } y_i \\neq \\hat{y}_i \n",
    "\t\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "4. Perda SVM/Hinge\n",
    " \n",
    "$$\\frac{1}{N} \\sum_{i=1}^N \\max(0, 1- y^{h}_i\\cdot f(x_i)),$$\n",
    "essa função considera que as classes são -1 e 1, sendo $f(x_i)=\\hat{y}_i^{h}$ um valor de saída considerando valores negativos (os quais gerarão classificação para a classe -1) e positivos (classificação para a classe 1). Portanto será preciso adaptar as classes do problema e a  saída $\\hat{y}^{h}$ para esse cenário da seguinte forma:\n",
    "* $y^{h} \\in \\{-1,1\\}$, e\n",
    "* $\\hat{y}^{h} = 2\\cdot(\\hat{y}-0.5)$,\n",
    "sendo $\\hat{y}$ a probabilidade de uma instância pertencer à classe positiva (1).\n",
    "\n",
    "Considere o exemplo dado em aula, com os pontos unidimensionais conforme o código abaixo.\n",
    "\n",
    "A seguir, treine um classificador de Regressão Logística com solver `lbfgs` e compute as quatro perdas nesse conjunto de dados após o treinamento. Note que as perdas 1,2 e 4 são calculadas com base nas probabilidades, enquanto que 3 é calculada com base na classificação.\n",
    "\n",
    "Imprima as perdas por instância para inspeção e logo após a perda média no conjunto de treinamento. Qual a ordem de magnitude das perdas, da menor para a maior?\n",
    "\n",
    "(a) Hinge, Quadrática, Entropia Cruzada, 0-1<br>\n",
    "(b) Quadrática, Entropia Cruzada, Hinge e 0-1<br>\n",
    "(c) 0-1, Quadrática, Entropia Cruzada, Hinge<br>\n",
    "(d) Quadrática, 0-1, Entropia Cruzada, Hinge<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "6uamxcISyqpv",
    "outputId": "0bcb9cab-624c-41d0-fe3f-a908efb7c1a4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([-1.8,-1.5,-0.8,-0.4,-0.2, 0.0, 0.1, 0.5, 1.0, 1.3])\n",
    "y = np.array([ 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0,  1.0, 1.0, 1.0])\n",
    "yh = np.array([ -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "plt.scatter(x,np.zeros(10), c=y,cmap=plt.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OLlz13JyqrX"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 10)\n",
    "\n",
    "Carregue a base de dados Fashion MNIST\n",
    "\n",
    "Crie duas redes neurais utilizando os blocos Residuais e módulos Inception conforme visto em aula.\n",
    "\n",
    "* InceptionNet\n",
    "    * Módulo Inception V1 com número de filtros: 32, 32, 32, 32, 32, 16\n",
    "    * Maxpooling com pool=2, stride=2\n",
    "    * Módulo Inception V1 com número de filtros: 32, 64, 64, 64, 64, 16\n",
    "    * Maxpooling com pool=2, stride=2\n",
    "* ResNet\n",
    "    * 3 blocos residuais com 64 filtros, cada um seguido por camada Maxpooling com pool=2, stride=2\n",
    "\n",
    "Ambos devem possuir uma camada `GlobalAveragePooling2D` antes da camada de predição.\n",
    "\n",
    "Treine ambas com SGD, learning rate 0.05 e momentum 0.8, utilizando batchsize 32, e apenas as 1000 primeiras imagens do dataset de treinamento (use :1000), por 80 épocas. Antes de compilar e treinar cada modelo, defina as sementes numpy e tensorflow de forma fixa para 1.\n",
    "\n",
    "Exiba o gráfico da perda ao longo das épocas para as duas arquiteturas, e ao final compute e mostre a perda e a acurácia no treinamento (1000 imagens) e num conjunto de validação formado pelas próximas 1000 imagens de treinamento (use 1000:2000). \n",
    "\n",
    "Marque a alternativa que melhor se encaixa no resultado observado e sua conclusão.\n",
    "\n",
    "(a) Ambas convergem rapidamente para esse subconjunto e se ajustam aos dados de treinamento, mas falham em generalizar para o conjunto de validação<br>\n",
    "(b) A Inception converge mais rapidamente quando comparda à ResNet e se ajusta perfeitamente aos dados de treinamento, mas com perda mais alta calculada na validação, indicando overfitting, enquanto a ResNet generaliza melhor e poderia ser treinada por mais épocas<br>\n",
    "(c) A ResNet converge mais rapidamente quando comparada à Inception e se ajusta perfeitamente aos dados de treinamento, mas com perda mais alta calculada na validação, indicando overfitting, enquanto a Inception generaliza melhor e poderia ser treinada por mais épocas<br>\n",
    "(d) A ResNet converge mais rapidamente quando comparada à Inception e se ajusta perfeitamente aos dados de treinamento. Ambas possuem generalização similar o conjunto de validação.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "RVYNMoriyqrZ",
    "outputId": "b866fc9c-2d4e-425b-e849-29222f900bdd"
   },
   "outputs": [],
   "source": [
    "# carregando datasets do keras\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# obtendo informações das imagens (resolucao) e dos rótulos (número de classes)\n",
    "img_lin, img_col = x_train.shape[1], x_train.shape[2]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# dividir por 255 para obter normalizacao\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# transformar categorias em one-hot-encoding\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# verifica imagens da base de dados tem 3 canais (RGB) ou apenas 1 (escala de cinza)\n",
    "if (len(x_train.shape) == 3):\n",
    "      n_channels = 1\n",
    "else:\n",
    "      n_channels = x_train.shape[3]\n",
    "\n",
    "# re-formata o array de forma a encontrar o formato da entrada (input_shape)\n",
    "# se a dimensão dos canais vem primeiro ou após a imagem\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], n_channels, img_lin, img_col)\n",
    "    x_test = x_test.reshape(x_test.shape[0], n_channels, img_lin, img_col)\n",
    "    input_shape = (n_channels, img_lin, img_col)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_lin, img_col, n_channels)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_lin, img_col, n_channels)\n",
    "    input_shape = (img_lin, img_col, n_channels)\n",
    "\n",
    "print(\"Shape: \", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-03-Exercicios_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
