{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"RNAP-03-Exercicios_solucoes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K8CDQUj8yqpq"},"source":["## MBA em Ciência de Dados\n","# Redes Neurais e Arquiteturas Profundas\n","\n","### <span style=\"color:darkred\">Módulo III - Arquiteturas de CNNS e treinamento de redes profundas</span>\n","\n","\n","### <span style=\"color:darkred\">Exercícios com soluções</span>\n","\n","Moacir Antonelli Ponti\n","\n","CeMEAI - ICMC/USP São Carlos\n","\n","---\n","\n","#### <span style=\"color:red\">Recomenda-se fortemente que os exercícios sejam feitos sem consultar as respostas antecipadamente.</span>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1u9fCYcMebDu"},"source":["---\n","\n","## Parte 1 - Exercícios Essenciais"]},{"cell_type":"markdown","metadata":{"id":"038CuS5syqqL"},"source":["---\n","\n","### Exercício 1)\n","\n","Considere as funções de custo: Perda Quadrática (MSE), Erro Absoluto (MAE), Perda 0-1, Perda Hinge/SVM, Entropia Cruzada. Para referência veja a definição da Perda 0-1 e Hinge, não vistas em aula, abaixo. Pesquise mais sobre essas caso necessário\n","\n","*. Perda 0-1\n","\n","$$\\frac{1}{N} \\sum_{i=1}^N \n","\\left\\{\n","\t\\begin{array}{ll}\n","\t\t0  & \\mbox{if } y_i = \\hat{y}_i \\\\\n","\t\t1 & \\mbox{if } y_i \\neq \\hat{y}_i \n","\t\\end{array}\n","\\right.$$\n","\n","*. Perda SVM/Hinge\n"," \n","$$\\frac{1}{N} \\sum_{i=1}^N \\max(0, 1- y^{h}_i\\cdot f(x_i)),$$\n","essa função considera que as classes são -1 e 1, sendo $f(x_i)=\\hat{y}_i^{h}$ um valor de saída considerando valores negativos (os quais gerarão classificação para a classe -1) e positivos (classificação para a classe 1). Portanto será preciso adaptar as classes do problema e a  saída $\\hat{y}^{h}$ para esse cenário da seguinte forma:\n","* $y^{h} \\in \\{-1,1\\}$, e\n","* $\\hat{y}^{h} = 2\\cdot(\\hat{y}-0.5)$,\n","sendo $\\hat{y}$ a probabilidade de uma instância pertencer à classe positiva (1).\n","\n","Para um determinado problema, como escolher qual delas utilizar no treinamento de uma rede neural?\n","\n","(a) Na dúvida escolher sempre a entropia cruzada, pois é a mais popular e considerada um padrão na literatura da área de redes neurais<br>\n","(b) É necessário considerar um subconjunto pequeno de exemplos e sempre testar todas as funções de custo disponíveis, só assim é possível ter certeza de que estaremos selecionando a função mais adequada para o problema em mãos<br>\n","<font color='red'>(c) Avaliar o problema em termos de suas saídas e os valores possíveis para a função de custo, selecionando para realizar experimentos aquelas que mais se adequem ao problema e seja conveniente para realizar otimização baseada em gradiente<br></font>\n","(d) Em geral, a entropia cruzada deve ser utilizada para problemas de classificação, e a perda quadrática para problemas de regressão, não sendo necessário investigar outras funções de custo pois são mais relevantes outros parâmetros como a taxa de aprendizado e o tamanho do batch<br>\n","\n","**Justificativa:** Apesar de mais popular, a entropia cruzada pode não funcionar bem em todos os cenários. Por outro lado, fazer uma busca exaustiva é impraticável. Assim, é preciso tomar uma decisão \"educada\" com base no problema em questão, os valores de saída, e selecionar um subconjunto de funções candidatas a serem investigadas para resolver o problema."]},{"cell_type":"markdown","metadata":{"id":"nJQ0-S3myqqL"},"source":["---\n","### Exercício 2)\n","\n","Considerando as funções de perda: entropia cruzada categórica e perda quadrática, qual é o valor das perdas para um exemplo arbitrário no caso o modelo considere as classes equiprováveis numa tarefa de classificação de 5 classes?\n","\n"," (a) <font color='red'>Entropia Cruzada = 1.6; Quadrática = 0.8</font><br>\n"," (b) Entropia Cruzada = 2.3; Quadrática = 0.8<br>\n"," (c) Entropia Cruzada = 1.6; Quadrática = 0.16<br>\n"," (d) Entropia Cruzada = 0.32; Quadrática = 0.8<br>\n","  \n","DICA: compare dois vetores de probabilidade, um com a classe real em *one-hot-encoding* e o outro exemplificando o caso equiprovável.\n"," \n"," **Justificativa**: veja código abaixo. Na média, numa inicialização aleatória, teríamos um classificador gerando um vetor de probabilidade com a distribuição aproximadamente uniforme, ou seja, todos os valores 0.2=1/5. Computando a entropia cruzada categórica, temos apenas o -log do valor predito para a classe verdadeira, enquanto que na quadrática, a soma dos erros cometidos ao longo do vetor."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmVBm5CByqqM","executionInfo":{"status":"ok","timestamp":1634163359420,"user_tz":180,"elapsed":13,"user":{"displayName":"Moacir Antonelli Ponti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gii-34vbZyib3WjjMBVbm8-88oDpqhrecIMH0SQGg=s64","userId":"09722981635546271521"}},"outputId":"00ef6cd1-7d38-4fae-aa59-c515963259f4"},"source":["import numpy as np \n","\n","y = np.array([.0, .0, .0, .0, 1.0])\n","yh = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n","\n","loss_ec = -np.sum((y*np.log(yh+.000001)))\n","loss_qu = np.sum(np.power((y-yh),2))\n","print(loss_ec)\n","print(loss_qu)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["1.6094329124466003\n","0.8000000000000002\n"]}]},{"cell_type":"markdown","metadata":{"id":"6exl-MrVyqqT"},"source":["---\n","### Exercício 3)\n","\n","Sobre os métodos de otimização, o que podemos dizer quando comparamos SGD e Adam?\n","\n"," <font color='red'>(a) Ambos realizam atualização iterativa dos parâmetros usando o gradiente, mas o Adam utiliza também o segundo momento do gradiente como ponderação</font><br>\n"," (b) O Adam pode ser considerado um caso particular do SGD, sendo ambos idênticos se usado SGD com Momentum e atrito de 0,99.<br>\n"," (c) Ambos realizam atualização iterativa dos parâmetros usando o gradiente, mas apenas SGD permite decaimento da taxa de aprendizado <br>\n"," (d) O Adam sempre obterá resultados melhores do que o SGD para qualquer rede neural profunda<br>\n"," \n","  \n"," **Justificativa**: o SGD utiliza o gradiente, enquanto o Adam computa o segundo momento como forma de ponderar a magnitude do passo. As outras alternativas são inválidas porque: Adam utiliza uma estratégia similar, mas não igual ao momentum, e também possui um tipo de taxa de aprendizado adaptativa; Adam também permite decaimento da taxa de aprendizado; finalmente, ainda que Adam seja um algoritmo de otimização mais sofisticado, não é possível dizer que um algoritmo de otimização será sempre melhor, em particular para cenários complexos. Note por exemplo que muitos modelos do estado da arte são treinados com SGD."]},{"cell_type":"markdown","metadata":{"id":"Yfnm0YgLyqqU"},"source":["---\n","\n","### Exercício 4)\n","\n","Dentre as alternativas, escolha a prática válida mais relevante ao projetar o treinamento de redes profundas\n","\n","(a) Inicializar todos os pesos com valores aleatórios e utilizar o maior número de instâncias possíveis no treinamento, garantindo que os hiperparâmetros com valor padrão obterão bons resultados<br>\n","(b) Utilizar sempre a função de custo entropia cruzada, para a qual é recomendado o uso do otimizador Adam e taxa de aprendizado com decaimento. Definir a melhor taxa de decaimento de forma a minimizar a diferença entre o custo de treinamento e validação<br>\n","<font color='red'>(c) Utilizar conjunto pequeno de instâncias para busca inicial de hiperparâmetros como: otimizador, taxa de aprendizado, momentum e tamanho de batch, e depois refinar a busca num conjunto maior com base em métricas obtidas nos conjuntos de validação e treinamento<br></font>\n","(d) Rezar para Yan LeCun, Yoshua Bengio, Geoffrey Hinton e Kunihiko Fukushima.\n","\n","\n"," **Justificativa**: nem sempre os valores padrão serão bons hiperparâmetros. Ainda que algumas escolhas sejam populares (como uso de Adam e Entropia Cruzada), o melhor é sempre realizar uma busca, ainda que grosseira com poucos dados, por parâmetros que se ajustem à arquitetura projetada. Se você acredita, rezar pode até te acalmar, mas não vai ajudar no treinamento da rede. Felizmente os 4 estão vivos, então tentar contatá-los no Twitter pode ser uma opção ;)\n"," "]},{"cell_type":"markdown","metadata":{"id":"J6VfUMk8yqqW"},"source":["---\n","\n","### Exercício 5)\n","\n","Qual a principal diferença das arquiteturas VGGNet, Inception e Residual Network com relação à suas camadas convolucionais?\n","\n","(a) A VGGNet possui camadas convolucionais com filtros de mesmo tamanho $3\\times3$, enquanto as outras arquiteturas, Inception e ResNet aplicam filtros $5\\times5$ ou com concatenação de mapas de ativação ao longo da rede<br>\n","(b) A rede Inception permite treinamento com maior número de camadas quanto comparada à VGGNet, que por sua vez permite treinamento com maior número de camadas quanto comparada à ResNet <br>\n","(c) A VGGNet possui camadas convolucionais sequenciais, eventualmente seguidas de MaxPooling, enquanto a ResNet computa mapas de ativação de com diferentes filtros, concatenando-os, e a Inception possui um módulo do tipo banco de filtros, que permite saltar para camadas futuras, facilitando o treinamento com mais camadas<br>\n","<font color='red'>(d) A VGGNet possui camadas convolucionais sequenciais, enquanto Inception possui camadas convolucionais paralelas, e ResNet tem mapas de ativação que desviam da lógica sequencial e pulam camadas<br></font>\n","\n"," **Justificativa**: Sua principal diferença é o fluxo durante a rede, sendo a VGG sequencial e as outras duas cujas ativações dão saltos (ResNet) ou possuem paralelismo (Inception). Alternativa (a) está errada pois ResNet não aplica filtros de tamanho maior do que 5x5, nem realiza concatenação de mapas de ativação (mas sim a soma); (b) é inválida pois a ResNet permite treinar com mais camadas do que a VGG; (c) está errada pois Inception não possui saltos nas camadas, nem ResNet possui concatenação de mapas. 1"]},{"cell_type":"markdown","metadata":{"id":"C1sh5GgYyqqY"},"source":["---\n","\n","### Exercício 6)\n","\n","Utilizando a biblioteca Keras, investige os hiperparâmetros relacionadas a learning rate na base de dados Boston Housing. Carregue a base de dados e normalize os atributos com z-score. Crie uma rede com camadas densas: 16, 8 e 1 (de saída), todas com ativação `relu`, função de custo `mse`, medindo também a `mae` como avaliação adicional.\n","\n","Iremos investigar o uso de decaimento de learning rate, a partir de um valor inicial estabelecido. Para isso vamos usar um conjunto de validação de 20% retirado a partir do conjunto de testes, e repetir 5 vezes, cada vez utilizando uma semente, de 1 até 5, conforme código base abaixo.\n","\n","Treine por 25 épocas com batchsize 32 e com o otimizador Adam, 2 arquiteturas diferentes:\n","\n","*A*. Uso dos parâmetros padrão<br>\n","*B*. Iniciando com learning rate 0.01 e decaimento exponencial de 0.05 a partir da época 5\n","\n","Posteriormente, treine os dois modelos, porém agora com o conjunto de treinamento completo e avalie no conjunto de teste.\n","\n","Considerando a média dos valores de erro (MSE e MAE) obtidos na validação arredondados para um número inteiro (ou seja, sem considerar as casas decimais), e posteriormente os mesmos erros quando treinado com o conjunto completo e avaliados no teste:\n","\n","(a) B obteve menores valores de erro (MSE e MAE) do que A na validação, B também tem MAE menor do que A no teste, mas ambos foram similares no MSE do teste<br>\n","<font color='red'>(b) B obteve menores valores de erro (MSE e MAE) do que A na validação e no teste<br></font>\n","(c) A obteve valores de erro MAE menores do que B, mas valores MSE maiores do que B na validação e no teste.<br>\n","(d) A obteve valores de erro MAE e MSE similares com B validação e no teste<br>\n","\n","**Justificativa**: Notar no código abaixo que A obteve no treinamento MSE e MAE maiores do que o modelo B, sendo que o modelo B também tem uma generalizacao ligeiramente melhor (menor diferenca entre as medidas no treinamento e validacao)."]},{"cell_type":"code","metadata":{"id":"K5Owfr6GyqqY"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.datasets import boston_housing\n","from numpy.random import seed\n","from tensorflow.random import set_seed\n","from sklearn.model_selection import train_test_split\n","\n","(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n","\n","mean = x_train.mean(axis=0)\n","std = x_train.std(axis=0)\n","\n","x_train -= mean\n","x_train /= std\n","\n","x_test -= mean\n","x_test /= std\n","\n","def my_dnn(input_shape):\n","    model = keras.Sequential()\n","    model.add(keras.layers.Dense(16, activation=\"relu\", input_shape=input_shape))\n","    model.add(keras.layers.Dense(8, activation=\"relu\"))\n","    model.add(keras.layers.Dense(1, activation=\"relu\"))\n","    return model\n","\n","def scheduler(epoch, lr):\n","    if epoch < 5:\n","        return lr\n","    else:\n","        return np.round(lr * tf.math.exp(-0.05),4)\n","    \n","callbacklr = keras.callbacks.LearningRateScheduler(scheduler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"78QcV0RSebD4","outputId":"8d972c15-943f-43bc-b5c5-e148c7824a99"},"source":["epochs = 25\n","\n","# arrays para conter os erros\n","mses = []\n","maes = []\n","\n","# para cada semente\n","for sd in range(1,6):\n","    x_trains, x_val, y_trains, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=sd)\n","    \n","    print(\"Seed\", sd)\n","    seed(sd)\n","    set_seed(sd)\n","    modelA = my_dnn((x_trains.shape[1],))\n","    modelA.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics=['mae'])\n","    modelA.fit(x_trains, y_trains, epochs=epochs, batch_size=32, verbose=0)\n","    \n","    seed(sd)\n","    set_seed(sd)\n","    modelB = my_dnn((x_trains.shape[1],))\n","    modelB.compile(optimizer=keras.optimizers.Adam(0.01), loss='mse', metrics=['mae'])\n","    modelB.fit(x_trains, y_trains, epochs=epochs, batch_size=32, callbacks=[callbacklr], verbose=0)\n","\n","    score = modelA.evaluate(x_val, y_val, verbose = 0)\n","    mses.append(score[0])\n","    maes.append(score[1])\n","    \n","    score = modelB.evaluate(x_val, y_val, verbose = 0)\n","    mses.append(score[0])\n","    maes.append(score[1])\n","    \n","# converte em array e refaz o formato para que fiquem 5 execucoes (linhas) e 2 modelos (A e B)\n","mses = np.array(mses)        \n","mses = mses.reshape((5,2))\n","\n","maes = np.array(maes)        \n","maes = maes.reshape((5,2))\n","\n","mean_mses = np.mean(mses, axis=0)\n","mean_maes = np.mean(maes, axis=0)\n","print('Validacao\\tMSE\\tMAE')\n","for met,mse1,mae1  in zip(['Default LR', 'Scheduling'], mean_mses, mean_maes):\n","       print(\"%s\\t%.0f\\t%.0f\" % (met, mse1,mae1))    "],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Seed 1\n","Seed 2\n","Seed 3\n","Seed 4\n","Seed 5\n","\t\tMSE\tMAE\n","Default LR\t110\t8\n","Scheduling\t15\t3\n"]}]},{"cell_type":"code","metadata":{"id":"AGRp5NKRebD7","outputId":"5bfb81df-e391-41ef-a25b-36eee383343d"},"source":["# avalia novamentem agora no teste!\n","seed(sd)\n","set_seed(sd)\n","modelA = my_dnn((x_train.shape[1],))\n","modelA.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics=['mae'])\n","modelA.fit(x_train, y_train, epochs=epochs, batch_size=32, verbose=0)\n","    \n","seed(sd)\n","set_seed(sd)\n","modelB = my_dnn((x_train.shape[1],))\n","modelB.compile(optimizer=keras.optimizers.Adam(0.01), loss='mse', metrics=['mae'])\n","modelB.fit(x_train, y_train, epochs=epochs, batch_size=32, callbacks=[callbacklr], verbose=0)\n","\n","score = modelA.evaluate(x_test, y_test, verbose = 0)\n","print('Default LR', score)\n","    \n","score = modelB.evaluate(x_test, y_test, verbose = 0)\n","print('Scheduling', score)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Default LR [51.425804138183594, 5.763202667236328]\n","Scheduling [22.39095115661621, 3.257490873336792]\n"]}]},{"cell_type":"markdown","metadata":{"id":"J_F2ODAmyqq4"},"source":["---\n","\n","### Exercício 7)\n","\n","Utilizando ainda a biblioteca Keras, investige o impacto do uso de parâmetros padrão de batchsize na base de dados Boston Housing, agora utilizando a mesma arquitetura da atividade anterior, com otimizador Adam, iniciando com learning rate 0.02 e decaimento exponencial de 0.05 a partir da época 6.\n","\n","Investige valores de batch = 2, 4, 8, 16, 32, 64, 128 e 256 executando por 15 épocas.\n","\n","Para isso vamos usar um conjunto de validação de 20% retirado a partir do conjunto de testes, e repetir 5 vezes, cada vez utilizando uma semente, de 1 até 5, conforme código base abaixo.\n","\n","Após o treinamento, avalie MSE nos dados de validação e imprima a média do MSE obtido para os diferentes valores de batchsize.\n","\n","Quais foram os dois piores e os dois melhores valores de tamanho de batch em termos do MSE de validação?\n","\n","<font color=\"red\">(a) Piores: 128 e 256; Melhores: 8 e 16<br></font>\n","(b) Piores: 16 e 64; Melhores: 32 e 64<br>\n","(c) Piores: 2 e 4; Melhores: 8 e 16<br>\n","(d) Piores: 2 e 256; Melhores: 16 e 32<br>\n","\n","OBS: a aleatoriedade do processo pode gerar alguma variação de resultado, caso seja isso tente escolher a alternativa mais coerente, ou rodar algumas vezes para ver se há alternativa válida.\n","\n","**Justificativa**: Notar no código abaixo que a comparação. Há um ponto ótimo para o batch-size"]},{"cell_type":"code","metadata":{"id":"-29F6a8kyqq5"},"source":["def my_dnn(input_shape):\n","    model = keras.Sequential()\n","    model.add(keras.layers.Dense(16, activation=\"relu\", input_shape=input_shape))\n","    model.add(keras.layers.Dense(8, activation=\"relu\"))\n","    model.add(keras.layers.Dense(1, activation=\"relu\"))\n","    return model\n","\n","def scheduler(epoch, lr):\n","    if epoch <= 5:\n","        return lr\n","    else:\n","        return np.round(lr * tf.math.exp(-0.05),4)\n","    \n","callbacklr = keras.callbacks.LearningRateScheduler(scheduler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61grHxROyqrD"},"source":["epochs = 15\n","\n","batches = [2, 4, 8, 16, 32, 64, 128, 256]\n","\n","batch_error = []\n","for sd in range(1,6):\n","    seed(sd)\n","    set_seed(sd)\n","    x_trains, x_val, y_trains, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=sd)\n","    \n","    print(\"Seed\", sd, end=': ')\n","    for batch_size in batches:\n","        print(batch_size, end=' ')\n","        seed(sd)\n","        set_seed(sd)\n","        \n","        model = my_dnn((x_trains.shape[1],))\n","        model.compile(optimizer=keras.optimizers.Adam(0.02), loss='mse')\n","\n","        history = model.fit(x_trains, y_trains, epochs=epochs, batch_size=batch_size,\n","                             callbacks=[callbacklr], verbose=0)\n","\n","        score = model.evaluate(x_val, y_val, verbose = 0)\n","        batch_error.append(score)\n","        \n","    print('')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEfA8AZUebD-"},"source":["batch_error = np.array(batch_error)        \n","batch_error = batch_error.reshape((5,len(batches)))\n","\n","mean_error_batches = np.mean(batch_error, axis=0)\n","for bs, me in zip(batches, mean_error_batches):\n","       print(\"Batch size = %d, Erros de Validação MSE = %.4f\" % (bs, me))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mG0AgoULebD-"},"source":["---\n","\n","## Parte 2 - Exercícios Complementares"]},{"cell_type":"markdown","metadata":{"id":"Q3zlC0i0yqrX"},"source":["---\n","\n","### Exercício 8)\n","\n","O que podemos concluir dos dois exercícios anteriores (7 e 8)?\n","\n","(a) Os valores padrão para os hiperparâmetros geram bons resultados. A busca por outros parâmetros pode não valer a pena pois a diferença alcançada observada é pequena.<br>\n","(b) Devemos sempre utilizar Adam com decaimento de taxa de aprendizado e batch size de tamanho entre 8 e 64, sendo que o uso do padrão (32) é normalmente suficiente.<br>\n","(c) Batchs de tamanho muito grande são prejudiciais ao treinamento, e o otimizador Adam é sempre melhor com decaimento de taxa de aprendizado.<br>\n","<font color='red'>(d) O uso de hiperparâmetros com valores padrão pode gerar resultados subótimos, sendo importante uma busca de parâmetros para melhor otimizar modelos<br></font>"]},{"cell_type":"markdown","metadata":{"id":"xMJ4IFd7yqpt"},"source":[" ### Exercício 9)\n","\n","Considere 4 funções de custo distintas: 1. entropia cruzada binária, 2. perda quadrática, vistas em aula, e mais duas adicionais:\n","\n","3. Perda 0-1\n","\n","$$\\frac{1}{N} \\sum_{i=1}^N \n","\\left\\{\n","\t\\begin{array}{ll}\n","\t\t0  & \\mbox{if } y_i = \\hat{y}_i \\\\\n","\t\t1 & \\mbox{if } y_i \\neq \\hat{y}_i \n","\t\\end{array}\n","\\right.$$\n","\n","4. Perda SVM/Hinge\n"," \n","$$\\frac{1}{N} \\sum_{i=1}^N \\max(0, 1- y^{h}_i\\cdot f(x_i)),$$\n","essa função considera que as classes são -1 e 1, sendo $f(x_i)=\\hat{y}_i^{h}$ um valor de saída considerando valores negativos (os quais gerarão classificação para a classe -1) e positivos (classificação para a classe 1). Portanto será preciso adaptar as classes do problema e a  saída $\\hat{y}^{h}$ para esse cenário da seguinte forma:\n","* $y^{h} \\in \\{-1,1\\}$, e\n","* $\\hat{y}^{h} = 2\\cdot(\\hat{y}-0.5)$,\n","sendo $\\hat{y}$ a probabilidade de uma instância pertencer à classe positiva (1).\n","\n","Considere o exemplo dado em aula, com os pontos unidimensionais conforme o código abaixo.\n","\n","A seguir, treine um classificador de Regressão Logística com solver `lbfgs` e compute as quatro perdas nesse conjunto de dados após o treinamento. Note que as perdas 1,2 e 4 são calculadas com base nas probabilidades, enquanto que 3 é calculada com base na classificação.\n","\n","Imprima as perdas por instância para inspeção e logo após a perda média no conjunto de treinamento. Qual a ordem de magnitude das perdas, da menor para a maior?\n","\n","(a) Hinge, Quadrática, Entropia Cruzada, 0-1<br>\n","(b) Quadrática, Entropia Cruzada, Hinge e 0-1<br>\n","(c) 0-1, Quadrática, Entropia Cruzada, Hinge<br>\n","<font color='red'>(d) Quadrática, 0-1, Entropia Cruzada, Hinge</font><br>"]},{"cell_type":"code","metadata":{"id":"6uamxcISyqpv"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","x = np.array([-1.8,-1.5,-0.8,-0.4,-0.2, 0.0, 0.1, 0.5, 1.0, 1.3])\n","y = np.array([ 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0,  1.0, 1.0, 1.0])\n","yh = np.array([ -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0])\n","\n","plt.scatter(x,np.zeros(10), c=y,cmap=plt.cm.coolwarm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWg5eHSXyqp7"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","# treinando o modelo\n","logr1 = LogisticRegression(solver='lbfgs')\n","logr1.fit(x.reshape(-1, 1), y)\n","\n","# pegando as probabilidades de saída\n","y_hat = logr1.predict_proba(x.reshape(-1, 1))[:, 1].ravel()\n","print('y1    = {}'.format(np.round(y , 3)))\n","print('y_hat = {}'.format(np.round(y_hat, 3)))\n","\n","# classificando para calcular a perda 0-1\n","y_clas = y_hat.copy()\n","y_clas[y_clas>=0.5] = 1\n","y_clas[y_clas<0.5] = 0\n","\n","# calculando a saída hinge, entre -1 e 1\n","y_hat_hi = (y_hat-0.5)*2\n","print('y_hat_hi= {}'.format(np.round(y_hat_hi, 3)))\n","\n","# perda quadrática\n","loss_qu = np.power(y-y_hat,2)\n","# perda de entropia cruzada\n","loss_ec = -(y*np.log(y_hat+.00001) + (1-y)*np.log(1-y_hat +.00001))\n","# perda zero-um\n","loss_01 = (y!=y_clas)*1\n","\n","# perda hinge\n","hi_mult = 1-(yh*y_hat_hi)\n","loss_hi = [max(0,mi) for mi in hi_mult]\n","print('1-y*y_hat= {}'.format(np.round(hi_mult, 3)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k2jBVOEzyqqD"},"source":["print(\"Perdas calculadas por instância:\")\n","print(np.round(loss_qu,3))\n","print(np.round(loss_ec,3))\n","print(np.round(loss_01,3))\n","print(np.round(loss_hi,3))\n","print()\n","\n","print(\"Perda quadrática = %.4f\" % (np.mean(loss_qu)))\n","print(\"Entropia cruzada = %.4f\" % (np.mean(loss_ec)))\n","print(\"Perda 0-1        = %.4f\" % (np.mean(loss_01)))\n","print(\"Perda hinge/svm  = %.4f\" % (np.mean(loss_hi)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9OLlz13JyqrX"},"source":["---\n","\n","### Exercício 10)\n","\n","Carregue a base de dados Fashion MNIST\n","\n","Crie duas redes neurais utilizando os blocos Residuais e módulos Inception conforme visto em aula.\n","\n","* InceptionNet\n","    * Módulo Inception V1 com número de filtros: 32, 32, 32, 32, 32, 16\n","    * Maxpooling com pool=2, stride=2\n","    * Módulo Inception V1 com número de filtros: 32, 64, 64, 64, 64, 16\n","    * Maxpooling com pool=2, stride=2\n","* ResNet\n","    * 3 blocos residuais com 64 filtros, cada um seguido por camada Maxpooling com pool=2, stride=2\n","\n","Ambos devem possuir uma camada `GlobalAveragePooling2D` antes da camada de predição.\n","\n","Treine ambas com SGD, learning rate 0.05 e momentum 0.8, utilizando batchsize 32, e apenas as 1000 primeiras imagens do dataset de treinamento (use :1000), por 80 épocas. Antes de compilar e treinar cada modelo, defina as sementes numpy e tensorflow de forma fixa para 1.\n","\n","Exiba o gráfico da perda ao longo das épocas para as duas arquiteturas, e ao final compute e mostre a perda e a acurácia no treinamento (1000 imagens) e num conjunto de validação formado pelas próximas 1000 imagens de treinamento (use 1000:2000). \n","\n","Marque a alternativa que melhor se encaixa no resultado observado e sua conclusão.\n","\n","(a) Ambas convergem rapidamente para esse subconjunto e se ajustam aos dados de treinamento, mas falham em generalizar para o conjunto de validação<br>\n","(b) A Inception converge mais rapidamente quando comparda à ResNet e se ajusta perfeitamente aos dados de treinamento, mas com perda mais alta calculada na validação, indicando overfitting, enquanto a ResNet generaliza melhor e poderia ser treinada por mais épocas<br>\n","<font color='red'>(c) A ResNet converge mais rapidamente quando comparada à Inception e se ajusta perfeitamente aos dados de treinamento, mas com perda mais alta calculada na validação, indicando overfitting, enquanto a Inception generaliza melhor e poderia ser treinada por mais épocas<br></font>\n","(d) A ResNet converge mais rapidamente quando comparada à Inception e se ajusta perfeitamente aos dados de treinamento. Ambas possuem generalização similar o conjunto de validação.<br>\n","\n","**Justificativa**: A ResNet converge muito rapidamente para o custo próximo a zero, e portanto indica overfitting com 80 épocas. Ao final das mesmas épocas a Inception ainda não se aproximou de zero, indicando que poderia ser treinada por mais épocas ou ter sua otimização ajustada. A Inception possui generalização melhor do que a ResNet, visto que tanto o custo quanto a acurácia de treinamento e validação obtidos pela Inception são mais similares."]},{"cell_type":"code","metadata":{"id":"RVYNMoriyqrZ"},"source":["# carregando datasets do keras\n","#from tensorflow.keras.datasets import mnist\n","\n","from tensorflow.keras.datasets import fashion_mnist\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","\n","# obtendo informações das imagens (resolucao) e dos rótulos (número de classes)\n","img_lin, img_col = x_train.shape[1], x_train.shape[2]\n","num_classes = len(np.unique(y_train))\n","\n","print(x_train.shape)\n","\n","# dividir por 255 para obter normalizacao\n","x_train = x_train.astype('float32') / 255.0\n","x_test = x_test.astype('float32') / 255.0\n","\n","# transformar categorias em one-hot-encoding\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","# verifica imagens da base de dados tem 3 canais (RGB) ou apenas 1 (escala de cinza)\n","if (len(x_train.shape) == 3):\n","      n_channels = 1\n","else:\n","      n_channels = x_train.shape[3]\n","\n","# re-formata o array de forma a encontrar o formato da entrada (input_shape)\n","# se a dimensão dos canais vem primeiro ou após a imagem\n","if keras.backend.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], n_channels, img_lin, img_col)\n","    x_test = x_test.reshape(x_test.shape[0], n_channels, img_lin, img_col)\n","    input_shape = (n_channels, img_lin, img_col)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], img_lin, img_col, n_channels)\n","    x_test = x_test.reshape(x_test.shape[0], img_lin, img_col, n_channels)\n","    input_shape = (img_lin, img_col, n_channels)\n","\n","print(\"Shape: \", input_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJ3ZeShByqrd"},"source":["from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import concatenate\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.layers import add\n","\n","def inception_module(layer_in, f1_out, f2_in, f2_out, f3_in, f3_out, f4_out):\n","    # 1x1 conv\n","    conv1 = Conv2D(f1_out, (1,1), padding='same', activation='relu')(layer_in)\n","    # 3x3 conv\n","    conv3 = Conv2D(f2_in, (1,1), padding='same', activation='relu')(layer_in)\n","    conv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu')(conv3)\n","    # 5x5 conv\n","    conv5 = Conv2D(f3_in, (1,1), padding='same', activation='relu')(layer_in)\n","    conv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu')(conv5)\n","    # 3x3 max pooling\n","    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n","    pool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(pool)\n","    layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n","    return layer_out\n"," \n","# define model input\n","input_layer = Input(shape=input_shape)\n","# add inception blocks\n","layer1 = inception_module(input_layer, 32, 32, 32, 32, 32, 16)\n","pool1 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer1) #\n","layer2 = inception_module(pool1, 32, 64, 64, 64, 64, 16) # rem?\n","pool2 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer2)\n","flatt = keras.layers.GlobalAveragePooling2D()(pool2)\n","\n","softmax = keras.layers.Dense(num_classes, activation='softmax')(flatt)\n","\n","# create model\n","Inception = keras.models.Model(inputs=input_layer, outputs=softmax)\n","# summarize model\n","Inception.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"caop8lv8yqrh"},"source":["def residual_block(layer_in, n_filters):\n","    merge_input = layer_in\n","    #verifica se é necessária uma primeira camada para deixar o número de filtros iguais para adição\n","    if layer_in.shape[-1] != n_filters:\n","        merge_input = Conv2D(n_filters, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n","    # conv1\n","    conv1 = Conv2D(n_filters, (3,3), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n","    # conv2\n","    conv2 = Conv2D(n_filters, (3,3), padding='same', activation='linear', kernel_initializer='he_normal')(conv1)\n","    # soma entrada com saída (pulou 2 camadas)\n","    layer_out = add([conv2, merge_input])\n","    # função de ativação da saída do bloco\n","    layer_out = keras.layers.Activation('relu')(layer_out)\n","    return layer_out\n"," \n","# define model input\n","visible = Input(shape=input_shape)\n","\n","layer1 = residual_block(visible, 64)\n","pool1 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer1)\n","layer2 = residual_block(pool1, 64)\n","pool2 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer2)\n","layer3 = residual_block(pool2, 64)\n","pool3 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer3)\n","flatt = keras.layers.GlobalAveragePooling2D()(pool3)\n","softmax = keras.layers.Dense(num_classes, activation='softmax')(flatt)\n","\n","# create model\n","ResNet = keras.models.Model(inputs=visible, outputs=softmax)\n","# summarize model\n","ResNet.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ey3iyaJFyqrk"},"source":["x_sub = x_train[:1000]\n","y_sub = y_train[:1000]\n","\n","x_val = x_train[1000:2000]\n","y_val = y_train[1000:2000]\n","\n","batch_size = 32\n","epochs = 80"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsboR6ZTyqrp"},"source":["# as sementes ajudam a ter resultados reproduzíveis\n","#tf.keras.backend.clear_session()\n","seed(1)\n","set_seed(1)\n","\n","Inception.compile(loss='categorical_crossentropy',\n","              optimizer=keras.optimizers.SGD(learning_rate=0.02, momentum=0.8),\n","              metrics=['accuracy'])\n","\n","histInc = Inception.fit(x_sub, y_sub,\n","                    batch_size=batch_size,\n","                    epochs=epochs, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"InGNAeBOyqrs"},"source":["# as sementes ajudam a ter resultados reproduzíveis\n","#tf.keras.backend.clear_session()\n","seed(1)\n","set_seed(1)\n","\n","ResNet.compile(loss='categorical_crossentropy',\n","              optimizer=keras.optimizers.SGD(lr=0.02, momentum=0.8),\n","              metrics=['accuracy'])\n","\n","histResNet = ResNet.fit(x_sub, y_sub,\n","                    batch_size=batch_size,\n","                    epochs=epochs, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICUijS3gyqrw"},"source":["score1T = Inception.evaluate(x_sub, y_sub, verbose = 0)\n","score2T = ResNet.evaluate(x_sub, y_sub, verbose = 0)\n","\n","score1 = Inception.evaluate(x_val, y_val, verbose = 0)\n","score2 = ResNet.evaluate(x_val, y_val, verbose = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VK0Nrv5kyqr1"},"source":["print(\"Inception Treinamento = Loss %.3f, Accuracy %.3f\" % (score1T[0], score1T[1]))\n","print(\"Inception Validação   = Loss %.3f, Accuracy %.3f\" % (score1[0], score1[1]))\n","\n","print(\"ResNet Treinamento    = Loss %.3f, Accuracy %.3f\" % (score2T[0], score2T[1]))\n","print(\"ResNet Validação      = Loss %.3f, Accuracy %.3f\" % (score2[0], score2[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0bJI1XO5XI2Y"},"source":["plt.plot(histResNet.history['loss'], label='ResNet')\n","plt.plot(histInc.history['loss'], label='Inception')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nflwu6NcebEF"},"source":[""],"execution_count":null,"outputs":[]}]}