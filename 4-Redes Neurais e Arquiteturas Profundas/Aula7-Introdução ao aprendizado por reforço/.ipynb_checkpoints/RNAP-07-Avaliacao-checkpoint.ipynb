{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VII -  Introdução ao Aprendizado por Reforço</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Avaliação</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Questão 1)\n",
    "\n",
    "Qual das alternativas melhor descreve o objetivo de um modelo de aprendizado por reforço?\n",
    "\n",
    "(a) Aprender a predizer o próximo estado com base em uma ação atual em determinado ambiente<br>\n",
    "(b) Aprender a agrupar ações com base na recompensa que oferecem<br>\n",
    "(c) Aprender uma política que permita obter agentes que mapeiem o ambiente de forma a minimizar o erro num conjunto de treinamento formado por ações<br>\n",
    "(d) Aprender boas estratégias para tomar a melhor ação possível em cada situação<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 2)\n",
    "\n",
    "Considere um agente em um ambiente, em determinado estado e possuindo um espaço de ações disponíveis. Assumindo que um algoritmo de aprendizado por reforço que já obteve aprendizado por um número de iterações arbitrário. Esse algoritmo escolherá uma ação com base:\n",
    "\n",
    " (a) Numa política ou uma tabela de recompensas com base em estados e ações<br>\n",
    " (b) Na predição de uma rede neural profunda usada como modelo auxiliar<br>\n",
    " (c) Na última observação fornecida pelo ambiente após a ação executada no instante anterior<br>\n",
    " (d) Em uma escolha completamente aleatória, visto que aprendizado por reforçco utiliza tentativa e erro<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Questão 3)\n",
    "\n",
    "Suponha que utilizamos uma rede neural para otimização de aprendizado por reforço. No cenário de Policy Learning e Value Learning, dado como entrada um estado atual, a saída da rede neural é projetada de forma a?\n",
    "\n",
    " (b) Policy: regressão, predizendo a recompensa no formato de probabilidades. Value: classificação, selecionando a ação de maior valor futuro; <br>\n",
    " (c) Policy: classificação, selecionando a ação que obterá a maior recompensa. Value: classificação, selecionando a ação de maior valor futuro<br></font>\n",
    " (d) Policy: regressão, predizendo a recompensa no formato de probabilidades. Value: regressão, estimando a recompensa de cada ação disponível<br>\n",
    " (d) Policy: classificação, selecionando a ação que obterá a maior recompensa. Value: regressão, estimando a recompensa de cada ação disponível<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 4)\n",
    "\n",
    "Usando as bibliotecas `gym`, `gym_algorithmic` e `gym_toytext` carregue os ambientes \"MountainCarContinuous-v0\" cujo objetivo é levar um carro até o topo de um monte, \"CartPole-v1\" cujo objetivo é equilibrar um poste sobre um carro e \"Roulette-v0\" que simula um jogo de roleta.\n",
    "\n",
    "Como é o espaço de ações possíveis desses ambientes?\n",
    "\n",
    "(a) MountainCarContinuous: contínuo com 3 ações sendo 1 negativa, uma positiva e uma tupla, CartPole: contínuo com uma ação entre -1 e 1, Roulette: discreto com 3 ações possíveis<br>\n",
    "(b) MountainCarContinuous: contínuo com uma ação com valor entre -1 e 1, CartPole: discreto com duas ações possíveis, Roulette: discreto com 38 ações possíveis<br>\n",
    "(c) MountainCarContinuous: contínuo com 3 ações sendo 1 negativa, uma positiva e uma tupla, CartPole: contínuo com duas ações possíveis, Roulette: discreto com 3 ações possíveis<br>\n",
    "(d) MountainCarContinuous: contínuo com uma ação com valor entre -1 e 1, CartPole: discreto com 38 ações possíveis, Roulette: discreto com duas ações possíveis<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym-legacy-toytext  #tem q ser esse senao da pau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym-algorithmic gym-legacy-toytext\n",
    "import gym\n",
    "import gym_algorithmic, gym_toytext\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1.], [1.], (1,), float32)\n",
      "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "#MountainCarContinuous-v0\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "#CartPole-v1\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(38)\n",
      "Discrete(1)\n"
     ]
    }
   ],
   "source": [
    "#Roulette-v0\n",
    "env = gym.make(\"Roulette-v0\")\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Questão 5)\n",
    "\n",
    "Carregue o ambiente `Blackjack-v1`. Esse problema gera recompensa +1 para vitória, 0 para empate, e -1 para derrota. Inicialize o ambiente, execute 50 mil episódios (cada um até o estado terminal) e calcule a média de ações por episódio (MA), a porcentagem de vitórias (PV), de derrotas (PD) e de empates (PE), sendo vitórias (recompensa = 1), derrotas (recompensa = -1) e empates (recompensa = 0) medidas após o final de cada episódio. Arredonde o valor de MA para 2 casas decimais e considere apenas a parte inteira das porcentagens PV, PE, PD.\n",
    "\n",
    "Qual o resultado de MA e como se relacionam os valores obtidos de PD, PE e PV?\n",
    "\n",
    "(a) MA está entre 1 e 2; PD > PE > PV<br>\n",
    "(b) MA está entre 0 e 1; PD = PE = PV<br>\n",
    "(c) MA está entre 0 e 1; PD > PE > PV<br>\n",
    "(d) MA está entre 1 e 2; PD = PE = PV<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)#duas acoes disponiveis 0 e 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\")\n",
    "\n",
    "n_episodios_teste = 50000\n",
    "total_epochs = 0\n",
    "total_acao=0\n",
    "total_recs = 0\n",
    "total_vit = 0\n",
    "total_der = 0\n",
    "total_emp = 0\n",
    "recompensas=[]\n",
    "\n",
    "for i in range(n_episodios_teste):\n",
    "    # resetar ambiente\n",
    "    observ0 = env.reset()\n",
    "    fim = False\n",
    "    # realizar episodio ate o fim\n",
    "    while not fim:\n",
    "        # obtem acao aleatoriamente\n",
    "        acao = env.action_space.sample()\n",
    "        total_acao+=acao\n",
    "        # realizar acao e obtem valores\n",
    "        #fim=True chegou no estado terminal\n",
    "        #rec=recompensa\n",
    "        #obs=observação\n",
    "        obs, rec, fim, info = env.step(acao)\n",
    "        total_epochs += 1\n",
    "        recompensas.append(rec)\n",
    "\n",
    "    # contar estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média das açãoes é 0.6905\n"
     ]
    }
   ],
   "source": [
    "print('Média das açãoes é {}'.format(total_acao/n_episodios_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(recompensas,columns=['recompensas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>49.365503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>30.180515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>20.453983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              %\n",
       "-1.0  49.365503\n",
       " 0.0  30.180515\n",
       " 1.0  20.453983"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.recompensas.value_counts(normalize=True).to_frame('%')*100"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Avaliacao_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
